{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clipping of NetCDF to Shape File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys #user to create and modify file name and save it\n",
    "from netCDF4 import Dataset#to read the .nc files\n",
    "import pandas as pd #for dataframe activities\n",
    "from datetime import datetime,timedelta #for the time related activities\n",
    "import matplotlib.pyplot as plt #for the Ploting purpose\n",
    "import numpy as np #numerical fucntions\n",
    "import datetime \n",
    "import warnings #to ignore unnecessary warnings\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import regionmask\n",
    "import geopandas as gp\n",
    "import glob                                                                 \n",
    "from pyhdf.SD import SD, SDC\n",
    "from zmq import ctx_opt_names #for the Ploting purpose\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Day of the year Calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime,date\n",
    "d=date(2007,5,31)\n",
    "dayofyear=d.timetuple().tm_yday\n",
    "print(dayofyear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining function to correct the taken dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_correction(DATAFIELD):\n",
    "    data2D = hdf.select(DATAFIELD)\n",
    "    data = data2D[:,:].astype(np.double)\n",
    "\n",
    "    # Retrieve attributes.\n",
    "    attrs = data2D.attributes(full=1)\n",
    "    aoa=attrs[\"add_offset\"]\n",
    "    add_offset = aoa[0]\n",
    "    fva=attrs[\"_FillValue\"]\n",
    "    _FillValue = fva[0]\n",
    "    sfa=attrs[\"scale_factor\"]\n",
    "    scale_factor = sfa[0]        \n",
    "    ua=attrs[\"units\"]\n",
    "    units = ua[0]\n",
    "    data[data == _FillValue] = np.nan\n",
    "    data = (data - add_offset) * scale_factor \n",
    "    datam = np.ma.masked_array(data, np.isnan(data))\n",
    "    return datam\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loop to read all the files and clip them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Volumes/PtatoBasket/Datasets/Unclipped_MYD/PRE-MONSOON/2005-2006')#For changing the wokring directory\n",
    "for file in list(glob.glob('MYD08*.hdf')):\n",
    "\n",
    "    reader = open(file)\n",
    "    hdf = SD(file, SDC.READ)\n",
    "\n",
    "    # Read geolocation dataset.\n",
    "    lat = hdf.select('YDim')\n",
    "    latitude = lat[:]\n",
    "    lon = hdf.select('XDim')\n",
    "    longitude = lon[:]\n",
    "\n",
    "    #Getting the dates from filename\n",
    "    arr=str(file).split(\".\")\n",
    "    year=str(arr[1])[1:5]\n",
    "    days=str(arr[1])[5:]\n",
    "    dat=datetime(int(year), 1, 1) + timedelta(int(days) - 1)\n",
    "    dat=(dat-datetime(2005,1,1)).days\n",
    "    \n",
    "    #Callingout the pre-defined function for the correction\n",
    "    cer = data_correction('Cloud_Effective_Radius_Liquid_Mean')\n",
    "    cot=data_correction('Cloud_Optical_Thickness_Combined_Mean')\n",
    "    ctt=data_correction('Cloud_Top_Temperature_Mean')\n",
    "    ctp=data_correction('Cloud_Top_Pressure_Mean')\n",
    "\n",
    "    # writing the ncfile\n",
    "    df = xr.Dataset(\n",
    "            data_vars={'Cloud_Effective_Radius_Liquid_Mean':    (('latitude', 'longitude' ), cer),\n",
    "                        'Cloud_Optical_Thickness_Combined_Mean':(('latitude', 'longitude' ), cot),\n",
    "                        'Cloud_Top_Temperature_Mean':(('latitude', 'longitude' ), ctt),\n",
    "                        'Cloud_Top_Pressure_Mean':(('latitude', 'longitude' ), ctp),\n",
    "                    },\n",
    "            coords={'longitude': longitude,\n",
    "                'latitude': latitude,\n",
    "                'time':dat,\n",
    "            })\n",
    "\n",
    "    df.Cloud_Effective_Radius_Liquid_Mean.attrs['long_name'] = 'Cloud Effective Radius'\n",
    "    df.Cloud_Optical_Thickness_Combined_Mean.attrs['long_name'] = 'Cloud Optical Radius'\n",
    "    df.Cloud_Top_Temperature_Mean.attrs['long_name'] = 'Cloud Top Temperautre'\n",
    "    df.Cloud_Top_Pressure_Mean.attrs['long_name'] = 'Cloud Top Pressure'\n",
    "\n",
    "    # defining the coordinates\n",
    "    df.longitude.attrs['standard_name'] = 'longitude'\n",
    "    df.longitude.attrs['long_name'] = 'longitude'\n",
    "    df.longitude.attrs['units'] = 'degrees_east'\n",
    "    df.longitude.attrs['axis'] = 'X'\n",
    "\n",
    "    df.latitude.attrs['standard_name'] = 'latitude'\n",
    "    df.latitude.attrs['long_name'] = 'latitude'\n",
    "    df.latitude.attrs['units'] = 'degrees_north'\n",
    "    df.latitude.attrs['axis'] = 'Y'\n",
    "\n",
    "\n",
    "    df.time.attrs['standard_name'] = 'time'\n",
    "    df.time.attrs['long_name'] = 'time'\n",
    "    df.time.attrs['axis'] = 'T'\n",
    "    df.time.attrs[\"units\"] = \"days since 2005-01-01\"\n",
    "\n",
    "    #Opening the already created mask file\n",
    "    mask=xr.open_dataarray('/Volumes/ACIML/Main/BOB_MASK.nc')\n",
    "    masked_shape=df.where(mask==0)\n",
    "\n",
    "    min_lon = 78.00\n",
    "    max_lon = 96.00\n",
    "    min_lat = 8.00\n",
    "    max_lat = 23.00\n",
    "\n",
    "    cropped_ds = masked_shape.sel(latitude=slice(max_lat,min_lat), longitude=slice(min_lon,max_lon))#Croppping the masked dataset to our range\n",
    "    fname=str(file)[:-3]\n",
    "    cropped_ds.to_netcdf('/Volumes/ACIML/Clipped-MOD/'+fname+'nc')#saving the final masked out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the shape file and making a mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapefile =\"/Volumes/ACIML/Main/SHPs/BOB_OCEAN/BOB_OCEAN.shp\"\n",
    "countries=gp.read_file(shapefile,engine='pyogrio')\n",
    "\n",
    "fig, ax =plt.subplots(figsize=(16,10))\n",
    "countries.plot(ax=ax,column='featurecla') \n",
    "c_list=list(countries['featurecla'])\n",
    "c_list_unique=set(list(countries['featurecla']))\n",
    "indexes=[c_list.index(x) for x in c_list_unique]\n",
    "countries_mask_poly=regionmask.Regions(outlines=countries.geometry[indexes],name='featurecla',numbers=indexes,names=countries.featurecla[indexes])\n",
    "print(\"{}\".format(countries_mask_poly.names[0]))\n",
    "\n",
    "#Clipping to shape File and Creating Mask File\n",
    "shapefile =\"/Volumes/ACIML/Main/SHPs/BOB_OCEAN/BOB_OCEAN.shp\"\n",
    "countries=gp.read_file(shapefile,engine='pyogrio')\n",
    "c_list=list(countries['featurecla'])\n",
    "c_list_unique=set(list(countries['featurecla']))\n",
    "indexes=[c_list.index(x) for x in c_list_unique]\n",
    "countries_mask_poly=regionmask.Regions(outlines=countries.geometry[indexes],name='featurecla',numbers=indexes,names=countries.featurecla[indexes])\n",
    "mask=countries_mask_poly.mask(df,lat_name='latitude',lon_name='longitude')\n",
    "mask.to_netcdf('BOB_MASK.nc')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('CLIMATE_SCIENCE')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "07b86caa93fadaa61f04e6b621513b8b6f4b7cc85bf00bd26e84ad3c3830db61"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
